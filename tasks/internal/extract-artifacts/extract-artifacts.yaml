---
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: extract-artifacts
  labels:
    app.kubernetes.io/version: "0.0.1"
  annotations:
    tekton.dev/pipelines.minVersion: "0.12.1"
    tekton.dev/tags: release
spec:
  description: >-
    Tekton task that extracts binaries using oras and saves them in a workspace
  params:
    - name: snapshot_json
      type: string
      description: String containing a JSON representation of the snapshot spec
    - name: concurrentLimit
      type: string
      description: The maximum number of images to be pulled at once
      default: 3
  results:
    - name: result
      type: string
      description: Success if the task succeeds, the error otherwise
  workspaces:
    - name: data
      description: The workspace where pulled binaries are stored
  steps:
    - name: extract-artifacts
      image: quay.io/konflux-ci/release-service-utils:7835e32b1974f956a6c942e24adbc79705cab12e
      env:
        - name: DOCKER_CONFIG_JSON
          valueFrom:
            secretKeyRef:
              name: redhat-workloads-token
              key: .dockerconfigjson
        - name: "SNAPSHOT_JSON"
          value: "$(params.snapshot_json)"
      script: |
        #!/usr/bin/env bash
        set -ex

        STDERR_FILE=/tmp/stderr.txt

        exitfunc() {
            local err=$1
            local line=$2
            local command="$3"
            if [ "$err" -eq 0 ] ; then
                echo -n "Success" > "$(results.result.path)"
            else
                echo "$0: ERROR '$command' failed at line $line - exited with status $err" \
                  > "$(results.result.path)"
                if [ -f "$STDERR_FILE" ] ; then
                    tail -n 20 "$STDERR_FILE" >> "$(results.result.path)"
                fi
            fi
            exit 0 # exit the script cleanly as there is no point in proceeding past an error or exit call
        }
        # due to set -e, this catches all EXIT and ERR calls and the task should never fail with nonzero exit code
        trap 'exitfunc $? $LINENO "$BASH_COMMAND"' EXIT

        DISK_IMAGE_DIR="$(workspaces.data.path)/artifacts"
        export DISK_IMAGE_DIR
        mkdir -p "$DISK_IMAGE_DIR"

        process_component() { # Expected argument is [component json]
            COMPONENT=$1
            PULLSPEC=$(jq -er '.containerImage' <<< "${COMPONENT}")
            DESTINATION="${DISK_IMAGE_DIR}/$(jq -er '.staged.destination' <<< "${COMPONENT}")/FILES" \
              || (echo "Missing staged.destination value for component. This should be an existing pulp repo. \
                  Failing" && exit 1)
            mkdir -p "${DESTINATION}"
            DOWNLOAD_DIR=$(mktemp -d)
            cd "$DOWNLOAD_DIR"
            # oras has very limited support for selecting the right auth entry,
            # so create a custom auth file with just one entry
            AUTH_FILE=$(mktemp)
            select-oci-auth "${PULLSPEC}" > "$AUTH_FILE"
            oras pull --registry-config "$AUTH_FILE" "$PULLSPEC"
            NUM_MAPPED_FILES=$(jq '.staged.files | length' <<< "${COMPONENT}")
            for ((i = 0; i < NUM_MAPPED_FILES; i++)) ; do
                FILE=$(jq -c --arg i "$i" '.staged.files[$i|tonumber]' <<< "$COMPONENT")
                SOURCE=$(jq -er '.source' <<< "$FILE")
                FILENAME=$(jq -er '.filename' <<< "$FILE")
                # The .qcow2 images are not zipped
                if [ -f "${SOURCE}.gz" ] ; then
                    gzip -d "${SOURCE}.gz"
                fi
                DESTINATION_FILE="${DESTINATION}/${FILENAME}"
                # Albeit a low probability, a race condition can occur since this is run in parallel.
                # The race condition is if two files have the same $DESTINATION_FILE and both
                # if checks are run before either mv is run a few lines below.
                if [ -f "${DESTINATION_FILE}" ] ; then
                    echo -n "Multiple files use the same destination value: $DESTINATION" >&2
                    echo " and filename value: $FILENAME. Failing..." >&2
                    exit 1
                fi
                mv "$SOURCE" "${DESTINATION_FILE}" || echo "didn't find mapped file: ${SOURCE}"
            done

        }

        RUNNING_JOBS="\j" # Bash parameter for number of jobs currently running
        NUM_COMPONENTS=$(jq '.components | length' <<< "$SNAPSHOT_JSON")

        # use the 1st component's version
        VERSION=$(jq -cr '.components[0].staged.version // ""' <<< "$SNAPSHOT_JSON")
        if [ "${VERSION}" == "" ] ; then
          echo "Error: version not specified in .components[0].staged.version. Needed to publish to customer portal"
          exit 1
        fi

        # Pull each component in parallel
        for ((i = 0; i < NUM_COMPONENTS; i++)) ; do
            COMPONENT=$(jq -c --arg i "$i" '.components[$i|tonumber]' <<< "$SNAPSHOT_JSON")
            # Limit batch size to concurrent limit
            while (( ${RUNNING_JOBS@P} >= $(params.concurrentLimit) )); do
                wait -n
            done
            process_component "$COMPONENT" 2> "$STDERR_FILE" &
        done

        # Wait for remaining processes to finish
        while (( ${RUNNING_JOBS@P} > 0 )); do
            wait -n
        done
